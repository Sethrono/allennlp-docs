

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>allennlp.nn.util &mdash; AllenNLP 0.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="AllenNLP 0.0 documentation" href="../index.html"/>
        <link rel="up" title="allennlp.nn" href="allennlp.nn.html"/>
        <link rel="next" title="allennlp.service" href="allennlp.service.html"/>
        <link rel="prev" title="allennlp.nn.regularizers" href="allennlp.nn.regularizers.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/allennlp-logo-dark.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="allennlp.commands.html">allennlp.commands</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.subcommand.html">allennlp.commands.subcommand</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.evaluate.html">allennlp.commands.evaluate</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.predict.html">allennlp.commands.predict</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.serve.html">allennlp.commands.serve</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.train.html">allennlp.commands.train</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.common.html">allennlp.common</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.checks.html">allennlp.common.checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.file_utils.html">allennlp.common.file_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.params.html">allennlp.common.params</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.registrable.html">allennlp.common.registrable</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.squad_eval.html">allennlp.common.squad_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.tee_logger.html">allennlp.common.tee_logger</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.testing.html">allennlp.common.testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.util.html">allennlp.common.util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.data.html">allennlp.data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.dataset.html">allennlp.data.dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.dataset_readers.html">allennlp.data.dataset_readers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.dataset_reader.html">allennlp.data.dataset_readers.dataset_reader</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.conll2003.html">allennlp.data.dataset_readers.conll2003</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.language_modeling.html">allennlp.data.dataset_readers.language_modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.reading_comprehension.html">allennlp.data.dataset_readers.reading_comprehension</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.coreference_resolution.html">allennlp.data.dataset_readers.coreference_resolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.semantic_role_labeling.html">allennlp.data.dataset_readers.semantic_role_labeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.seq2seq.html">allennlp.data.dataset_readers.seq2seq</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.sequence_tagging.html">allennlp.data.dataset_readers.sequence_tagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.snli.html">allennlp.data.dataset_readers.snli</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.fields.html">allennlp.data.fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.instance.html">allennlp.data.instance</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.iterators.html">allennlp.data.iterators</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.token_indexers.html">allennlp.data.token_indexers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.tokenizers.html">allennlp.data.tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.vocabulary.html">allennlp.data.vocabulary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.models.html">allennlp.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.model.html">allennlp.models.model</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.archival.html">allennlp.models.archival</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.crf_tagger.html">allennlp.models.crf_tagger</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.decomposable_attention.html">allennlp.models.decomposable_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.encoder_decoders.html">allennlp.models.encoder_decoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.reading_comprehension.html">allennlp.models.reading_comprehension</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.coreference_resolution.html">allennlp.models.coreference_resolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.semantic_role_labeler.html">allennlp.models.semantic_role_labeler</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.simple_tagger.html">allennlp.models.simple_tagger</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.modules.html">allennlp.modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.attention.html">allennlp.modules.attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.augmented_lstm.html">allennlp.modules.augmented_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.lstm_cell_with_projection.html">allennlp.modules.lstm_cell_with_projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.elmo_lstm.html">allennlp.modules.elmo_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.conditional_random_field.html">allennlp.modules.conditional_random_field</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.feedforward.html">allennlp.modules.feedforward</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.highway.html">allennlp.modules.highway</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.matrix_attention.html">allennlp.modules.matrix_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2seq_encoders.html">allennlp.modules.seq2seq_encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2vec_encoders.html">allennlp.modules.seq2vec_encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.similarity_functions.html">allennlp.modules.similarity_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.stacked_alternating_lstm.html">allennlp.modules.stacked_alternating_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.text_field_embedders.html">allennlp.modules.text_field_embedders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.time_distributed.html">allennlp.modules.time_distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.token_embedders.html">allennlp.modules.token_embedders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.scalar_mix.html">allennlp.modules.scalar_mix</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="allennlp.nn.html">allennlp.nn</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.activations.html">allennlp.nn.activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.initializers.html">allennlp.nn.initializers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.regularizers.html">allennlp.nn.regularizers</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">allennlp.nn.util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.service.html">allennlp.service</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.db.html">allennlp.service.db</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.permalinks.html">allennlp.service.permalinks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.predictors.html">allennlp.service.predictors</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.server_sanic.html">allennlp.service.server_sanic</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.training.html">allennlp.training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.learning_rate_schedulers.html">allennlp.training.learning_rate_schedulers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.metrics.html">allennlp.training.metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.optimizers.html">allennlp.training.optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.trainer.html">allennlp.training.trainer</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">AllenNLP</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="allennlp.nn.html">allennlp.nn</a> &raquo;</li>
        
      <li>allennlp.nn.util</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api/allennlp.nn.util.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-allennlp.nn.util">
<span id="allennlp-nn-util"></span><h1>allennlp.nn.util<a class="headerlink" href="#module-allennlp.nn.util" title="Permalink to this headline">¶</a></h1>
<p>Assorted utilities for working with neural networks in AllenNLP.</p>
<dl class="function">
<dt id="allennlp.nn.util.add_sentence_boundary_token_ids">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">add_sentence_boundary_token_ids</code><span class="sig-paren">(</span><em>tensor: torch.FloatTensor</em>, <em>mask: torch.FloatTensor</em>, <em>sentence_begin_token: typing.Any</em>, <em>sentence_end_token: typing.Any</em><span class="sig-paren">)</span> &#x2192; typing.Tuple[torch.FloatTensor, torch.FloatTensor]<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L776-L829"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.add_sentence_boundary_token_ids" title="Permalink to this definition">¶</a></dt>
<dd><p>Add begin/end of sentence tokens to the batch of sentences.
Given a batch of sentences with size <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">timesteps)</span></code> or
<code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">timesteps,</span> <span class="pre">dim)</span></code> this returns a tensor of shape
<code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">timesteps</span> <span class="pre">+</span> <span class="pre">2)</span></code> or <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">timesteps</span> <span class="pre">+</span> <span class="pre">2,</span> <span class="pre">dim)</span></code> respectively.</p>
<p>Returns both the new tensor and updated mask.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>tensor</strong> : <code class="docutils literal"><span class="pre">torch.Tensor</span></code></p>
<blockquote>
<div><p>A tensor of shape <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">timesteps)</span></code> or <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">timesteps,</span> <span class="pre">dim)</span></code></p>
</div></blockquote>
<p><strong>mask</strong> : <code class="docutils literal"><span class="pre">torch.Tensor</span></code></p>
<blockquote>
<div><p>A tensor of shape <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">timesteps)</span></code></p>
</div></blockquote>
<p><strong>sentence_begin_token: Any (anything that can be broadcast in torch for assignment)</strong></p>
<blockquote>
<div><p>For 2D input, a scalar with the &lt;S&gt; id. For 3D input, a tensor with length dim.</p>
</div></blockquote>
<p><strong>sentence_end_token: Any (anything that can be broadcast in torch for assignment)</strong></p>
<blockquote>
<div><p>For 2D input, a scalar with the &lt;/S&gt; id. For 3D input, a tensor with length dim.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>tensor_with_boundary_tokens</strong> : <code class="docutils literal"><span class="pre">torch.Tensor</span></code></p>
<blockquote>
<div><p>The tensor with the appended and prepended boundary tokens. If the input was 2D,
it has shape (batch_size, timesteps + 2) and if the input was 3D, it has shape
(batch_size, timesteps + 2, dim).</p>
</div></blockquote>
<p><strong>new_mask</strong> : <code class="docutils literal"><span class="pre">torch.Tensor</span></code></p>
<blockquote class="last">
<div><p>The new mask for the tensor, taking into account the appended tokens
marking the beginning and end of the sentence.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.arrays_to_variables">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">arrays_to_variables</code><span class="sig-paren">(</span><em>data_structure: typing.Dict[str, typing.Union[dict, numpy.ndarray]], cuda_device: int = -1, add_batch_dimension: bool = False, for_training: bool = True</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L109-L156"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.arrays_to_variables" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert an (optionally) nested dictionary of arrays to Pytorch <code class="docutils literal"><span class="pre">Variables</span></code>,
suitable for use in a computation graph.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>data_structure</strong> : Dict[str, Union[dict, numpy.ndarray]], required.</p>
<blockquote>
<div><p>The nested dictionary of arrays to convert to Pytorch <code class="docutils literal"><span class="pre">Variables</span></code>.</p>
</div></blockquote>
<p><strong>cuda_device</strong> : int, optional (default = -1)</p>
<blockquote>
<div><p>If cuda_device &lt;= 0, GPUs are available and Pytorch was compiled with
CUDA support, the tensor will be copied to the cuda_device specified.</p>
</div></blockquote>
<p><strong>add_batch_dimension</strong> : bool, optional (default = False).</p>
<blockquote>
<div><p>Optionally add a batch dimension to tensors converted to <code class="docutils literal"><span class="pre">Variables</span></code>
using this function. This is useful during inference for passing
tensors representing a single example to a Pytorch model which
would otherwise not have a batch dimension.</p>
</div></blockquote>
<p><strong>for_training</strong> : <code class="docutils literal"><span class="pre">bool</span></code>, optional (default = <code class="docutils literal"><span class="pre">True</span></code>)</p>
<blockquote>
<div><p>If <code class="docutils literal"><span class="pre">False</span></code>, we will pass the <code class="docutils literal"><span class="pre">volatile=True</span></code> flag when constructing variables, which
disables gradient computations in the graph.  This makes inference more efficient
(particularly in memory usage), but is incompatible with training models.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The original data structure or tensor converted to a Pytorch <code class="docutils literal"><span class="pre">Variable</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.batched_index_select">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">batched_index_select</code><span class="sig-paren">(</span><em>target: torch.FloatTensor</em>, <em>indices: torch.LongTensor</em>, <em>flattened_indices: typing.Union[torch.LongTensor</em>, <em>NoneType] = None</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L640-L690"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.batched_index_select" title="Permalink to this definition">¶</a></dt>
<dd><p>The given <cite>indices</cite> of size <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">d_1,</span> <span class="pre">...,</span> <span class="pre">d_n)</span></code> indexes into the sequence dimension
(dimension 2) of the target, which has size <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">embedding_size)</span></code>.</p>
<p>This function returns selected values in the target with respect to the provided indices, which
have size <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">d_1,</span> <span class="pre">...,</span> <span class="pre">d_n,</span> <span class="pre">embedding_size)</span></code>. This can use the optionally precomputed
<code class="xref py py-func docutils literal"><span class="pre">flattened_indices()</span></code> with size <code class="docutils literal"><span class="pre">(batch_size</span> <span class="pre">*</span> <span class="pre">d_1</span> <span class="pre">*</span> <span class="pre">...</span> <span class="pre">*</span> <span class="pre">d_n)</span></code> if given.</p>
<p>An example use case of this function is looking up the start and end indices of spans in a
sequence tensor. This is used in the
<code class="xref py py-class docutils literal"><span class="pre">CoreferenceResolver</span></code>. Model to select
contextual word representations corresponding to the start and end indices of mentions. The
key reason this can&#8217;t be done with basic torch functions is that we want to be able to use
look-up tensors with an arbitrary number of dimensions (for example, in the coref model,
we don&#8217;t know a-priori how many spans we are looking up).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>target</strong> : <code class="docutils literal"><span class="pre">torch.Tensor</span></code>, required.</p>
<blockquote>
<div><p>A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size).
This is the tensor to be indexed.</p>
</div></blockquote>
<p><strong>indices</strong> : <code class="docutils literal"><span class="pre">torch.LongTensor</span></code></p>
<blockquote>
<div><p>A tensor of shape (batch_size, ...), where each element is an index into the
<code class="docutils literal"><span class="pre">sequence_length</span></code> dimension of the <code class="docutils literal"><span class="pre">target</span></code> tensor.</p>
</div></blockquote>
<p><strong>flattened_indices</strong> : Optional[torch.Tensor], optional (default = None)</p>
<blockquote>
<div><p>An optional tensor representing the result of calling :func:~`flatten_and_batch_shift_indices`
on <code class="docutils literal"><span class="pre">indices</span></code>. This is helpful in the case that the indices can be flattened once and
cached for many batch lookups.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>selected_targets</strong> : <code class="docutils literal"><span class="pre">torch.Tensor</span></code></p>
<blockquote class="last">
<div><p>A tensor with shape [indices.size(), target.size(-1)] representing the embedded indices
extracted from the batch flattened target tensor.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.bucket_values">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">bucket_values</code><span class="sig-paren">(</span><em>distances: torch.FloatTensor</em>, <em>num_identity_buckets: int = 4</em>, <em>num_total_buckets: int = 10</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L737-L774"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.bucket_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Places the given values (designed for distances) into <code class="docutils literal"><span class="pre">num_total_buckets``semi-logscale</span>
<span class="pre">buckets,</span> <span class="pre">with</span> <span class="pre">``num_identity_buckets</span></code> of these capturing single values.</p>
<p>The default settings will bucket values into the following buckets:
[0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+].</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>distances</strong> : <code class="docutils literal"><span class="pre">torch.Tensor</span></code>, required.</p>
<blockquote>
<div><p>A Tensor of any size, to be bucketed.</p>
</div></blockquote>
<p><strong>num_identity_buckets: int, optional (default = 4).</strong></p>
<blockquote>
<div><p>The number of identity buckets (those only holding a single value).</p>
</div></blockquote>
<p><strong>num_total_buckets</strong> : int, (default = 10)</p>
<blockquote>
<div><p>The total number of buckets to bucket values into.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A tensor of the same shape as the input, containing the indices of the buckets</p>
<p class="last">the values were placed in.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.combine_tensors">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">combine_tensors</code><span class="sig-paren">(</span><em>combination: str, tensors: typing.List[torch.FloatTensor]</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L482-L510"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.combine_tensors" title="Permalink to this definition">¶</a></dt>
<dd><p>Combines a list of tensors using element-wise operations and concatenation, specified by a
<code class="docutils literal"><span class="pre">combination</span></code> string.  The string refers to (1-indexed) positions in the input tensor list,
and looks like <code class="docutils literal"><span class="pre">&quot;1,2,1+2,3-1&quot;</span></code>.</p>
<p>We allow the following kinds of combinations: <code class="docutils literal"><span class="pre">x</span></code>, <code class="docutils literal"><span class="pre">x*y</span></code>, <code class="docutils literal"><span class="pre">x+y</span></code>, <code class="docutils literal"><span class="pre">x-y</span></code>, and <code class="docutils literal"><span class="pre">x/y</span></code>,
where <code class="docutils literal"><span class="pre">x</span></code> and <code class="docutils literal"><span class="pre">y</span></code> are positive integers less than or equal to <code class="docutils literal"><span class="pre">len(tensors)</span></code>.  Each of
the binary operations is performed elementwise.  You can give as many combinations as you want
in the <code class="docutils literal"><span class="pre">combination</span></code> string.  For example, for the input string <code class="docutils literal"><span class="pre">&quot;1,2,1*2&quot;</span></code>, the result
would be <code class="docutils literal"><span class="pre">[1;2;1*2]</span></code>, as you would expect, where <code class="docutils literal"><span class="pre">[;]</span></code> is concatenation along the last
dimension.</p>
<p>If you have a fixed, known way to combine tensors that you use in a model, you should probably
just use something like <code class="docutils literal"><span class="pre">torch.cat([x_tensor,</span> <span class="pre">y_tensor,</span> <span class="pre">x_tensor</span> <span class="pre">*</span> <span class="pre">y_tensor])</span></code>.  This
function adds some complexity that is only necessary if you want the specific combination used
to be <cite>configurable</cite>.</p>
<p>If you want to do any element-wise operations, the tensors involved in each element-wise
operation must have the same shape.</p>
<p>This function also accepts <code class="docutils literal"><span class="pre">x</span></code> and <code class="docutils literal"><span class="pre">y</span></code> in place of <code class="docutils literal"><span class="pre">1</span></code> and <code class="docutils literal"><span class="pre">2</span></code> in the combination
string.</p>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.device_mapping">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">device_mapping</code><span class="sig-paren">(</span><em>cuda_device: int</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L460-L471"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.device_mapping" title="Permalink to this definition">¶</a></dt>
<dd><p>In order to <cite>torch.load()</cite> a GPU-trained model onto a CPU (or specific GPU),
you have to supply a <cite>map_location</cite> function. Call this with
the desired <cite>cuda_device</cite> to get the function that <cite>torch.load()</cite> needs.</p>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.flatten_and_batch_shift_indices">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">flatten_and_batch_shift_indices</code><span class="sig-paren">(</span><em>indices: torch.FloatTensor</em>, <em>sequence_length: int</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L596-L637"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.flatten_and_batch_shift_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a subroutine for <a class="reference internal" href="#allennlp.nn.util.batched_index_select" title="allennlp.nn.util.batched_index_select"><code class="xref py py-func docutils literal"><span class="pre">batched_index_select()</span></code></a>. The given <code class="docutils literal"><span class="pre">indices</span></code> of size
<code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">d_1,</span> <span class="pre">...,</span> <span class="pre">d_n)</span></code> indexes into dimension 2 of a target tensor, which has size
<code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">embedding_size)</span></code>. This function returns a vector that
correctly indexes into the flattened target. The sequence length of the target must be
provided to compute the appropriate offsets.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
<span class="c1"># Sequence length of the target tensor.</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">shifted_indices</span> <span class="o">=</span> <span class="n">flatten_and_batch_shift_indices</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">)</span>
<span class="c1"># Indices into the second element in the batch are correctly shifted</span>
<span class="c1"># to take into account that the target tensor will be flattened before</span>
<span class="c1"># the indices are applied.</span>
<span class="k">assert</span> <span class="n">shifted_indices</span> <span class="o">==</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>indices</strong> : <code class="docutils literal"><span class="pre">torch.LongTensor</span></code>, required.</p>
<p><strong>sequence_length</strong> : <code class="docutils literal"><span class="pre">int</span></code>, required.</p>
<blockquote>
<div><p>The length of the sequence the indices index into.
This must be the second dimension of the tensor.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>offset_indices</strong> : <code class="docutils literal"><span class="pre">torch.LongTensor</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.flattened_index_select">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">flattened_index_select</code><span class="sig-paren">(</span><em>target: torch.FloatTensor</em>, <em>indices: torch.LongTensor</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L693-L722"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.flattened_index_select" title="Permalink to this definition">¶</a></dt>
<dd><p>The given <code class="docutils literal"><span class="pre">indices</span></code> of size <code class="docutils literal"><span class="pre">(set_size,</span> <span class="pre">subset_size)</span></code> specifies subsets of the <code class="docutils literal"><span class="pre">target</span></code>
that each of the set_size rows should select. The <cite>target</cite> has size
<code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">sequence_length,</span> <span class="pre">embedding_size)</span></code>, and the resulting selected tensor has size
<code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">set_size,</span> <span class="pre">subset_size,</span> <span class="pre">embedding_size)</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>target</strong> : <code class="docutils literal"><span class="pre">torch.Tensor</span></code>, required.</p>
<blockquote>
<div><p>A Tensor of shape (batch_size, sequence_length, embedding_size).</p>
</div></blockquote>
<p><strong>indices</strong> : <code class="docutils literal"><span class="pre">torch.LongTensor</span></code>, required.</p>
<blockquote>
<div><p>A LongTensor of shape (set_size, subset_size). All indices must be &lt; sequence_length
as this tensor is an index into the sequence_length dimension of the target.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>selected</strong> : <code class="docutils literal"><span class="pre">torch.Tensor</span></code>, required.</p>
<blockquote class="last">
<div><p>A Tensor of shape (batch_size, set_size, subset_size, embedding_size).</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.get_combined_dim">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">get_combined_dim</code><span class="sig-paren">(</span><em>combination: str, tensor_dims: typing.List[int]</em><span class="sig-paren">)</span> &#x2192; int<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L535-L554"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.get_combined_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>For use with <a class="reference internal" href="#allennlp.nn.util.combine_tensors" title="allennlp.nn.util.combine_tensors"><code class="xref py py-func docutils literal"><span class="pre">combine_tensors()</span></code></a>.  This function computes the resultant dimension when
calling <code class="docutils literal"><span class="pre">combine_tensors(combination,</span> <span class="pre">tensors)</span></code>, when the tensor dimension is known.  This is
necessary for knowing the sizes of weight matrices when building models that use
<code class="docutils literal"><span class="pre">combine_tensors</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>combination</strong> : <code class="docutils literal"><span class="pre">str</span></code></p>
<blockquote>
<div><p>A comma-separated list of combination pieces, like <code class="docutils literal"><span class="pre">&quot;1,2,1*2&quot;</span></code>, specified identically to
<code class="docutils literal"><span class="pre">combination</span></code> in <a class="reference internal" href="#allennlp.nn.util.combine_tensors" title="allennlp.nn.util.combine_tensors"><code class="xref py py-func docutils literal"><span class="pre">combine_tensors()</span></code></a>.</p>
</div></blockquote>
<p><strong>tensor_dims</strong> : <code class="docutils literal"><span class="pre">List[int]</span></code></p>
<blockquote class="last">
<div><p>A list of tensor dimensions, where each dimension is from the <cite>last axis</cite> of the tensors
that will be input to <a class="reference internal" href="#allennlp.nn.util.combine_tensors" title="allennlp.nn.util.combine_tensors"><code class="xref py py-func docutils literal"><span class="pre">combine_tensors()</span></code></a>.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.get_dropout_mask">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">get_dropout_mask</code><span class="sig-paren">(</span><em>dropout_probability: float</em>, <em>tensor_for_masking: torch.autograd.variable.Variable</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L82-L106"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.get_dropout_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes and returns an element-wise dropout mask for a given tensor, where
each element in the mask is dropped out with probability dropout_probability.
Note that the mask is NOT applied to the tensor - the tensor is passed to retain
the correct CUDA tensor type for the mask.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>dropout_probability</strong> : float, required.</p>
<blockquote>
<div><p>Probability of dropping a dimension of the input.</p>
</div></blockquote>
<p><strong>tensor_for_masking</strong> : torch.Variable, required.</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A torch.FloatTensor consisting of the binary mask scaled by 1/ (1 - dropout_probability).</p>
<p>This scaling ensures expected values and variances of the output of applying this mask</p>
<blockquote class="last">
<div><p>and the original tensor are the same.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.get_lengths_from_binary_sequence_mask">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">get_lengths_from_binary_sequence_mask</code><span class="sig-paren">(</span><em>mask: torch.FloatTensor</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L18-L34"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.get_lengths_from_binary_sequence_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute sequence lengths for each batch element in a tensor using a
binary mask.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>mask</strong> : torch.Tensor, required.</p>
<blockquote>
<div><p>A 2D binary mask of shape (batch_size, sequence_length) to
calculate the per-batch sequence lengths from.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A torch.LongTensor of shape (batch_size,) representing the lengths</p>
<p class="last">of the sequences in the batch.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.get_range_vector">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">get_range_vector</code><span class="sig-paren">(</span><em>size: int</em>, <em>is_cuda: bool</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L725-L734"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.get_range_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a range vector with the desired size, starting at 0. The CUDA implementation
is meant to avoid copy data from CPU to GPU.</p>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.get_text_field_mask">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">get_text_field_mask</code><span class="sig-paren">(</span><em>text_field_tensors: typing.Dict[str, torch.FloatTensor]</em><span class="sig-paren">)</span> &#x2192; torch.LongTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L284-L307"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.get_text_field_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes the dictionary of tensors produced by a <code class="docutils literal"><span class="pre">TextField</span></code> and returns a mask of shape
<code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">num_tokens)</span></code>.  This mask will be 0 where the tokens are padding, and 1
otherwise.</p>
<p>There could be several entries in the tensor dictionary with different shapes (e.g., one for
word ids, one for character ids).  In order to get a token mask, we assume that the tensor in
the dictionary with the lowest number of dimensions has plain token ids.  This allows us to
also handle cases where the input is actually a <code class="docutils literal"><span class="pre">ListField[TextField]</span></code>.</p>
<p>NOTE: Our functions for generating masks create torch.LongTensors, because using
torch.byteTensors inside Variables makes it easy to run into overflow errors
when doing mask manipulation, such as summing to get the lengths of sequences - see below.
&gt;&gt;&gt; mask = torch.ones([260]).byte()
&gt;&gt;&gt; mask.sum() # equals 260.
&gt;&gt;&gt; var_mask = torch.autograd.Variable(mask)
&gt;&gt;&gt; var_mask.sum() # equals 4, due to 8 bit precision - the sum overflows.</p>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.last_dim_log_softmax">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">last_dim_log_softmax</code><span class="sig-paren">(</span><em>tensor: torch.FloatTensor</em>, <em>mask: typing.Union[torch.FloatTensor</em>, <em>NoneType] = None</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L340-L346"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.last_dim_log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a tensor with 3 or more dimensions and does a masked log softmax over the last dimension.
We assume the tensor has shape <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">...,</span> <span class="pre">sequence_length)</span></code> and that the mask (if given)
has shape <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>.</p>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.last_dim_softmax">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">last_dim_softmax</code><span class="sig-paren">(</span><em>tensor: torch.FloatTensor</em>, <em>mask: typing.Union[torch.FloatTensor</em>, <em>NoneType] = None</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L331-L337"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.last_dim_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a tensor with 3 or more dimensions and does a masked softmax over the last dimension.  We
assume the tensor has shape <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">...,</span> <span class="pre">sequence_length)</span></code> and that the mask (if given)
has shape <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">sequence_length)</span></code>.</p>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.logsumexp">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">logsumexp</code><span class="sig-paren">(</span><em>tensor: torch.FloatTensor</em>, <em>dim: int = -1</em>, <em>keepdim: bool = False</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L572-L594"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.logsumexp" title="Permalink to this definition">¶</a></dt>
<dd><p>A numerically stable computation of logsumexp. This is mathematically equivalent to
<cite>tensor.exp().sum(dim, keep=keepdim).log()</cite>.  This function is typically used for summing log
probabilities.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>tensor</strong> : torch.FloatTensor, required.</p>
<blockquote>
<div><p>A tensor of arbitrary size.</p>
</div></blockquote>
<p><strong>dim</strong> : int, optional (default = -1)</p>
<blockquote>
<div><p>The dimension of the tensor to apply the logsumexp to.</p>
</div></blockquote>
<p><strong>keepdim: bool, optional (default = False)</strong></p>
<blockquote class="last">
<div><p>Whether to retain a dimension of size one at the dimension we reduce over.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.masked_log_softmax">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">masked_log_softmax</code><span class="sig-paren">(</span><em>vector</em>, <em>mask</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L181-L195"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.masked_log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal"><span class="pre">torch.nn.functional.log_softmax(vector)</span></code> does not work if some elements of <code class="docutils literal"><span class="pre">vector</span></code> should be
masked.  This performs a log_softmax on just the non-masked portions of <code class="docutils literal"><span class="pre">vector</span></code>.  Passing
<code class="docutils literal"><span class="pre">None</span></code> in for the mask is also acceptable; you&#8217;ll just get a regular log_softmax.</p>
<p>We assume that both <code class="docutils literal"><span class="pre">vector</span></code> and <code class="docutils literal"><span class="pre">mask</span></code> (if given) have shape <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">vector_dim)</span></code>.</p>
<p>In the case that the input vector is completely masked, this function returns an array
of <code class="docutils literal"><span class="pre">0.0</span></code>.  You should be masking the result of whatever computation comes out of this in that
case, anyway, so it shouldn&#8217;t matter.</p>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.masked_softmax">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">masked_softmax</code><span class="sig-paren">(</span><em>vector</em>, <em>mask</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L159-L178"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.masked_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal"><span class="pre">torch.nn.functional.softmax(vector)</span></code> does not work if some elements of <code class="docutils literal"><span class="pre">vector</span></code> should be
masked.  This performs a softmax on just the non-masked portions of <code class="docutils literal"><span class="pre">vector</span></code>.  Passing
<code class="docutils literal"><span class="pre">None</span></code> in for the mask is also acceptable; you&#8217;ll just get a regular softmax.</p>
<p>We assume that both <code class="docutils literal"><span class="pre">vector</span></code> and <code class="docutils literal"><span class="pre">mask</span></code> (if given) have shape <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">vector_dim)</span></code>.</p>
<p>In the case that the input vector is completely masked, this function returns an array
of <code class="docutils literal"><span class="pre">0.0</span></code>. This behavior may cause <code class="docutils literal"><span class="pre">NaN</span></code> if this is used as the last layer of a model
that uses categorical cross-entropy loss.</p>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.ones_like">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">ones_like</code><span class="sig-paren">(</span><em>tensor: torch.FloatTensor</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L474-L479"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.ones_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Use clone() + fill_() to make sure that a ones tensor ends up on the right
device at runtime.</p>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.replace_masked_values">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">replace_masked_values</code><span class="sig-paren">(</span><em>tensor: torch.autograd.variable.Variable</em>, <em>mask: torch.autograd.variable.Variable</em>, <em>replace_with: float</em><span class="sig-paren">)</span> &#x2192; torch.autograd.variable.Variable<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L445-L457"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.replace_masked_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Replaces all masked values in <code class="docutils literal"><span class="pre">tensor</span></code> with <code class="docutils literal"><span class="pre">replace_with</span></code>.  <code class="docutils literal"><span class="pre">mask</span></code> must be broadcastable
to the same shape as <code class="docutils literal"><span class="pre">tensor</span></code>. We require that <code class="docutils literal"><span class="pre">tensor.dim()</span> <span class="pre">==</span> <span class="pre">mask.dim()</span></code>, as otherwise we
won&#8217;t know which dimensions of the mask to unsqueeze.</p>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.sequence_cross_entropy_with_logits">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">sequence_cross_entropy_with_logits</code><span class="sig-paren">(</span><em>logits: torch.FloatTensor</em>, <em>targets: torch.LongTensor</em>, <em>weights: torch.FloatTensor</em>, <em>batch_average: bool = True</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L388-L442"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.sequence_cross_entropy_with_logits" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the cross entropy loss of a sequence, weighted with respect to
some user provided weights. Note that the weighting here is not the same as
in the <code class="xref py py-func docutils literal"><span class="pre">torch.nn.CrossEntropyLoss()</span></code> criterion, which is weighting
classes; here we are weighting the loss contribution from particular elements
in the sequence. This allows loss computations for models which use padding.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>logits</strong> : <code class="docutils literal"><span class="pre">torch.FloatTensor</span></code>, required.</p>
<blockquote>
<div><p>A <code class="docutils literal"><span class="pre">torch.FloatTensor</span></code> of size (batch_size, sequence_length, num_classes)
which contains the unnormalized probability for each class.</p>
</div></blockquote>
<p><strong>targets</strong> : <code class="docutils literal"><span class="pre">torch.LongTensor</span></code>, required.</p>
<blockquote>
<div><p>A <code class="docutils literal"><span class="pre">torch.LongTensor</span></code> of size (batch, sequence_length) which contains the
index of the true class for each corresponding step.</p>
</div></blockquote>
<p><strong>weights</strong> : <code class="docutils literal"><span class="pre">torch.FloatTensor</span></code>, required.</p>
<blockquote>
<div><p>A <code class="docutils literal"><span class="pre">torch.FloatTensor</span></code> of size (batch, sequence_length)</p>
</div></blockquote>
<p><strong>batch_average</strong> : bool, optional, (default = True).</p>
<blockquote>
<div><p>A bool indicating whether the loss should be averaged across the batch,
or returned as a vector of losses per batch element.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A torch.FloatTensor representing the cross entropy loss.</p>
<p>If <code class="docutils literal"><span class="pre">batch_average</span> <span class="pre">==</span> <span class="pre">True</span></code>, the returned loss is a scalar.</p>
<p class="last">If <code class="docutils literal"><span class="pre">batch_average</span> <span class="pre">==</span> <span class="pre">False</span></code>, the returned loss is a vector of shape (batch_size,).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.sort_batch_by_length">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">sort_batch_by_length</code><span class="sig-paren">(</span><em>tensor: torch.autograd.variable.Variable</em>, <em>sequence_lengths: torch.autograd.variable.Variable</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L37-L79"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.sort_batch_by_length" title="Permalink to this definition">¶</a></dt>
<dd><p>Sort a batch first tensor by some specified lengths.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>tensor</strong> : Variable(torch.FloatTensor), required.</p>
<blockquote>
<div><p>A batch first Pytorch tensor.</p>
</div></blockquote>
<p><strong>sequence_lengths</strong> : Variable(torch.LongTensor), required.</p>
<blockquote>
<div><p>A tensor representing the lengths of some dimension of the tensor which
we want to sort by.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>sorted_tensor</strong> : Variable(torch.FloatTensor)</p>
<blockquote>
<div><p>The original tensor sorted along the batch dimension with respect to sequence_lengths.</p>
</div></blockquote>
<p><strong>sorted_sequence_lengths</strong> : Variable(torch.LongTensor)</p>
<blockquote>
<div><p>The original sequence_lengths sorted by decreasing size.</p>
</div></blockquote>
<p><strong>restoration_indices</strong> : Variable(torch.LongTensor)</p>
<blockquote>
<div><p>Indices into the sorted_tensor such that
<code class="docutils literal"><span class="pre">sorted_tensor.index_select(0,</span> <span class="pre">restoration_indices)</span> <span class="pre">==</span> <span class="pre">original_tensor</span></code></p>
</div></blockquote>
<p><strong>permuation_index</strong> : Variable(torch.LongTensor)</p>
<blockquote class="last">
<div><p>The indices used to sort the tensor. This is useful if you want to sort many
tensors using the same ordering.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.viterbi_decode">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">viterbi_decode</code><span class="sig-paren">(</span><em>tag_sequence: torch.FloatTensor, transition_matrix: torch.FloatTensor, tag_observations: typing.Union[typing.List[int], NoneType] = None</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L198-L281"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.viterbi_decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Viterbi decoding in log space over a sequence given a transition matrix
specifying pairwise (transition) potentials between tags and a matrix of shape
(sequence_length, num_tags) specifying unary potentials for possible tags per
timestep.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>tag_sequence</strong> : torch.Tensor, required.</p>
<blockquote>
<div><p>A tensor of shape (sequence_length, num_tags) representing scores for
a set of tags over a given sequence.</p>
</div></blockquote>
<p><strong>transition_matrix</strong> : torch.Tensor, required.</p>
<blockquote>
<div><p>A tensor of shape (num_tags, num_tags) representing the binary potentials
for transitioning between a given pair of tags.</p>
</div></blockquote>
<p><strong>tag_observations</strong> : Optional[List[int]], optional, (default = None)</p>
<blockquote>
<div><p>A list of length <code class="docutils literal"><span class="pre">sequence_length</span></code> containing the class ids of observed
elements in the sequence, with unobserved elements being set to -1. Note that
it is possible to provide evidence which results in degenerate labellings if
the sequences of tags you provide as evidence cannot transition between each
other, or those transitions are extremely unlikely. In this situation we log a
warning, but the responsibility for providing self-consistent evidence ultimately
lies with the user.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>viterbi_path</strong> : List[int]</p>
<blockquote>
<div><p>The tag indices of the maximum likelihood tag sequence.</p>
</div></blockquote>
<p><strong>viterbi_score</strong> : float</p>
<blockquote class="last">
<div><p>The score of the viterbi path.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="allennlp.nn.util.weighted_sum">
<code class="descclassname">allennlp.nn.util.</code><code class="descname">weighted_sum</code><span class="sig-paren">(</span><em>matrix: torch.FloatTensor</em>, <em>attention: torch.FloatTensor</em><span class="sig-paren">)</span> &#x2192; torch.FloatTensor<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py#L349-L385"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.nn.util.weighted_sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes a matrix of vectors and a set of weights over the rows in the matrix (which we call an
&#8220;attention&#8221; vector), and returns a weighted sum of the rows in the matrix.  This is the typical
computation performed after an attention mechanism.</p>
<p>Note that while we call this a &#8220;matrix&#8221; of vectors and an attention &#8220;vector&#8221;, we also handle
higher-order tensors.  We always sum over the second-to-last dimension of the &#8220;matrix&#8221;, and we
assume that all dimensions in the &#8220;matrix&#8221; prior to the last dimension are matched in the
&#8220;vector&#8221;.  Non-matched dimensions in the &#8220;vector&#8221; must be <cite>directly after the batch dimension</cite>.</p>
<p>For example, say I have a &#8220;matrix&#8221; with dimensions <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">num_queries,</span> <span class="pre">num_words,</span>
<span class="pre">embedding_dim)</span></code>.  The attention &#8220;vector&#8221; then must have at least those dimensions, and could
have more. Both:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">num_queries,</span> <span class="pre">num_words)</span></code> (distribution over words for each query)</li>
<li><code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">num_documents,</span> <span class="pre">num_queries,</span> <span class="pre">num_words)</span></code> (distribution over words in a
query for each document)</li>
</ul>
</div></blockquote>
<p>are valid input &#8220;vectors&#8221;, producing tensors of shape:
<code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">num_queries,</span> <span class="pre">embedding_dim)</span></code> and
<code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">num_documents,</span> <span class="pre">num_queries,</span> <span class="pre">embedding_dim)</span></code> respectively.</p>
</dd></dl>

</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="allennlp.service.html" class="btn btn-neutral float-right" title="allennlp.service" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="allennlp.nn.regularizers.html" class="btn btn-neutral" title="allennlp.nn.regularizers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Allen Institute for Artificial Intelligence.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>