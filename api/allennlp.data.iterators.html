

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>allennlp.data.iterators &mdash; AllenNLP 0.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="AllenNLP 0.0 documentation" href="../index.html"/>
        <link rel="up" title="allennlp.data" href="allennlp.data.html"/>
        <link rel="next" title="allennlp.data.token_indexers" href="allennlp.data.token_indexers.html"/>
        <link rel="prev" title="allennlp.data.instance" href="allennlp.data.instance.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/allennlp-logo-dark.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="allennlp.commands.html">allennlp.commands</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.evaluate.html">allennlp.commands.evaluate</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.predict.html">allennlp.commands.predict</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.serve.html">allennlp.commands.serve</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.train.html">allennlp.commands.train</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.common.html">allennlp.common</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.checks.html">allennlp.common.checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.params.html">allennlp.common.params</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.registrable.html">allennlp.common.registrable</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.testing.html">allennlp.common.testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.util.html">allennlp.common.util</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="allennlp.data.html">allennlp.data</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.dataset.html">allennlp.data.dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.dataset_readers.html">allennlp.data.dataset_readers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.fields.html">allennlp.data.fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.instance.html">allennlp.data.instance</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">allennlp.data.iterators</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.token_indexers.html">allennlp.data.token_indexers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.tokenizers.html">allennlp.data.tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.vocabulary.html">allennlp.data.vocabulary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.models.html">allennlp.models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.model.html">allennlp.models.model</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.bidaf.html">allennlp.models.bidaf</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.decomposable_attention.html">allennlp.models.decomposable_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.semantic_role_labeler.html">allennlp.models.semantic_role_labeler</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.simple_tagger.html">allennlp.models.simple_tagger</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.modules.html">allennlp.modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.attention.html">allennlp.modules.attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.augmented_lstm.html">allennlp.modules.augmented_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.feedforward.html">allennlp.modules.feedforward</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.highway.html">allennlp.modules.highway</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.matrix_attention.html">allennlp.modules.matrix_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2seq_encoders.html">allennlp.modules.seq2seq_encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2vec_encoders.html">allennlp.modules.seq2vec_encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.similarity_functions.html">allennlp.modules.similarity_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.stacked_alternating_lstm.html">allennlp.modules.stacked_alternating_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.text_field_embedders.html">allennlp.modules.text_field_embedders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.time_distributed.html">allennlp.modules.time_distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.token_embedders.html">allennlp.modules.token_embedders</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.nn.html">allennlp.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.activations.html">allennlp.nn.activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.initializers.html">allennlp.nn.initializers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.util.html">allennlp.nn.util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.service.html">allennlp.service</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.predictors.html">allennlp.service.predictors</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.server_sanic.html">allennlp.service.server_sanic</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.training.html">allennlp.training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.learning_rate_schedulers.html">allennlp.training.learning_rate_schedulers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.metrics.html">allennlp.training.metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.optimizers.html">allennlp.training.optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.regularizers.html">allennlp.training.regularizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.trainer.html">allennlp.training.trainer</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">AllenNLP</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="allennlp.data.html">allennlp.data</a> &raquo;</li>
        
      <li>allennlp.data.iterators</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api/allennlp.data.iterators.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-allennlp.data.iterators">
<span id="allennlp-data-iterators"></span><h1>allennlp.data.iterators<a class="headerlink" href="#module-allennlp.data.iterators" title="Permalink to this headline">¶</a></h1>
<p>The various <a class="reference internal" href="#allennlp.data.iterators.data_iterator.DataIterator" title="allennlp.data.iterators.data_iterator.DataIterator"><code class="xref py py-class docutils literal"><span class="pre">DataIterator</span></code></a> subclasses
can be used to iterate over datasets with different batching and padding schemes.</p>
<ul class="simple">
<li><a class="reference internal" href="#data-iterator"><span class="std std-ref">DataIterator</span></a></li>
<li><a class="reference internal" href="#adaptive-iterator"><span class="std std-ref">AdaptiveIterator</span></a></li>
<li><a class="reference internal" href="#basic-iterator"><span class="std std-ref">BasicIterator</span></a></li>
<li><a class="reference internal" href="#bucket-iterator"><span class="std std-ref">BucketIterator</span></a></li>
</ul>
<span class="target" id="module-allennlp.data.iterators.data_iterator"><span id="data-iterator"></span></span><dl class="class">
<dt id="allennlp.data.iterators.data_iterator.DataIterator">
<em class="property">class </em><code class="descclassname">allennlp.data.iterators.data_iterator.</code><code class="descname">DataIterator</code><a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/data_iterator.py#L14-L75"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.data_iterator.DataIterator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="allennlp.common.registrable.html#allennlp.common.registrable.Registrable" title="allennlp.common.registrable.Registrable"><code class="xref py py-class docutils literal"><span class="pre">allennlp.common.registrable.Registrable</span></code></a></p>
<p>An abstract <code class="docutils literal"><span class="pre">DataIterator</span></code> class. <code class="docutils literal"><span class="pre">DataIterators</span></code> must implement __call__, which yields
batched examples.</p>
<dl class="attribute">
<dt id="allennlp.data.iterators.data_iterator.DataIterator.default_implementation">
<code class="descname">default_implementation</code><em class="property"> = 'bucket'</em><a class="headerlink" href="#allennlp.data.iterators.data_iterator.DataIterator.default_implementation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="classmethod">
<dt id="allennlp.data.iterators.data_iterator.DataIterator.from_params">
<em class="property">classmethod </em><code class="descname">from_params</code><span class="sig-paren">(</span><em>params: allennlp.common.params.Params</em><span class="sig-paren">)</span> &#x2192; allennlp.data.iterators.data_iterator.DataIterator<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/data_iterator.py#L69-L75"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.data_iterator.DataIterator.from_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.iterators.data_iterator.DataIterator.get_num_batches">
<code class="descname">get_num_batches</code><span class="sig-paren">(</span><em>dataset: allennlp.data.dataset.Dataset</em><span class="sig-paren">)</span> &#x2192; int<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/data_iterator.py#L46-L52"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.data_iterator.DataIterator.get_num_batches" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of batches that <code class="docutils literal"><span class="pre">dataset</span></code> will be split into; if you want to track
progress through the batch with the generator produced by <code class="docutils literal"><span class="pre">__call__</span></code>, this could be
useful.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.iterators.adaptive_iterator"><span id="adaptive-iterator"></span></span><dl class="class">
<dt id="allennlp.data.iterators.adaptive_iterator.AdaptiveIterator">
<em class="property">class </em><code class="descclassname">allennlp.data.iterators.adaptive_iterator.</code><code class="descname">AdaptiveIterator</code><span class="sig-paren">(</span><em>adaptive_memory_usage_constant: float, padding_memory_scaling: typing.Callable[typing.Dict[str, typing.Dict[str, int]], float], maximum_batch_size: int = 10000, biggest_batch_first: bool = False, batch_size: int = None, sorting_keys: typing.List[typing.Tuple[str, str]] = None, padding_noise: float = 0.2</em><span class="sig-paren">)</span> &#x2192; None<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/adaptive_iterator.py#L17-L157"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.adaptive_iterator.AdaptiveIterator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.iterators.bucket_iterator.BucketIterator" title="allennlp.data.iterators.bucket_iterator.BucketIterator"><code class="xref py py-class docutils literal"><span class="pre">allennlp.data.iterators.bucket_iterator.BucketIterator</span></code></a></p>
<p>An <code class="docutils literal"><span class="pre">AdaptiveIterator</span></code> is a <code class="docutils literal"><span class="pre">DataIterator</span></code> that varies the batch size to try to optimize
GPU memory usage.  Because padding lengths are done dynamically, we can have larger batches
when padding lengths are smaller, maximizing our usage of the GPU.  This is intended only for
use with very large models that only barely fit on the GPU - if your model is small enough that
you can easily fit a reasonable batch size on the GPU for your biggest instances, you probably
should just use a <code class="xref py py-class docutils literal"><span class="pre">BucketIterator</span></code>.  This is also still largely experimental, because it
interacts with the learning rate in odd ways, and we haven&#8217;t yet implemented good algorithms to
modify the learning rate based on batch size, etc.</p>
<p>In order for this to work correctly, you need to do two things:</p>
<ol class="arabic simple">
<li>Provide the <code class="docutils literal"><span class="pre">padding_memory_scaling</span></code> function, which gives a big-O bound on memory
usage given padding lengths. For instance, if you have two TextFields with
<code class="docutils literal"><span class="pre">sentence_lengths</span></code> which require padding, this might be simply <code class="docutils literal"><span class="pre">|sentence1|</span> <span class="pre">*</span>
<span class="pre">|sentence2|</span></code>.</li>
<li>Tune the <cite>adaptive_memory_usage_constant</cite> parameter for your particular model and GPU.
While tuning this, set <code class="docutils literal"><span class="pre">biggest_batch_first</span></code> to <code class="docutils literal"><span class="pre">True</span></code>, which will bypass the adaptive
grouping step and use the batching of a <code class="docutils literal"><span class="pre">BucketIterator</span></code>, returning the biggest batch
first.  You want to find the largest batch size for which this largest batch actually fits
on the GPU without running out of memory.  TODO(mattg): make this happen automatically
somehow.</li>
</ol>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>adaptive_memory_usage_constant</strong> : int, required.</p>
<blockquote>
<div><p>Only relevant if <code class="docutils literal"><span class="pre">use_adaptive_grouping</span></code> is <code class="docutils literal"><span class="pre">True</span></code>.  This is a manually-tuned parameter,
specific to a particular model architecture and amount of GPU memory (e.g., if you change
the number of hidden layers in your model, this number will need to change). The recommended
way to tune this parameter is to (1) use a fixed batch size, with <code class="docutils literal"><span class="pre">biggest_batch_first</span></code>
set to <code class="docutils literal"><span class="pre">True</span></code>, and find out the maximum batch size you can handle on your biggest instances
without running out of memory.  Then (2) turn on <code class="docutils literal"><span class="pre">use_adaptive_grouping</span></code>, and set this
parameter so that you get the right batch size for your biggest instances.  If you set the
log level to <code class="docutils literal"><span class="pre">DEBUG</span></code> in <code class="docutils literal"><span class="pre">scripts/run_model.py</span></code>, you can see the batch sizes that are
computed.</p>
</div></blockquote>
<p><strong>padding_memory_scaling: Callable[[Dict[str, Dict[str, int]]], float], required.</strong></p>
<blockquote>
<div><p>This function is used for computing the adaptive batch sizes.  We assume that memory usage
is a function that looks like this: <span class="math">\(M = b * O(p) * c\)</span>, where <span class="math">\(M\)</span> is the memory
usage, <span class="math">\(b\)</span> is the batch size, <span class="math">\(c\)</span> is some constant that depends on how much GPU
memory you have and various model hyperparameters, and <span class="math">\(O(p)\)</span> is a function outlining
how memory usage asymptotically varies with the padding lengths.  Our approach will be to
let the user effectively set <span class="math">\(\frac{M}{c}\)</span> using the <code class="docutils literal"><span class="pre">adaptive_memory_usage_constant</span></code>
above. This function specifies <span class="math">\(O(p)\)</span>, so we can solve for the batch size <span class="math">\(b\)</span>.
The more specific you get in specifying <span class="math">\(O(p)\)</span> in this function, the better a job we
can do in optimizing memory usage.</p>
</div></blockquote>
<p><strong>maximum_batch_size</strong> : int, optional (default=10000)</p>
<blockquote>
<div><p>If we&#8217;re using adaptive batch sizes, you can use this to be sure you do not create batches
larger than this, even if you have enough memory to handle it on your GPU.  You might
choose to do this to keep smaller batches because you like the noisier gradient estimates
that come from smaller batches, for instance.</p>
</div></blockquote>
<p><strong>biggest_batch_first</strong> : bool, optional (default=False)</p>
<blockquote>
<div><p>See <code class="xref py py-class docutils literal"><span class="pre">BucketIterator</span></code>.  If this is <code class="docutils literal"><span class="pre">True</span></code>, we bypass the adaptive grouping step, so
you can tune the <code class="docutils literal"><span class="pre">adaptive_memory_usage_constant</span></code>.</p>
</div></blockquote>
<p><strong>batch_size</strong> : int, optional (default=None)</p>
<blockquote>
<div><p>Only used when <code class="docutils literal"><span class="pre">biggest_batch_first</span></code> is <code class="docutils literal"><span class="pre">True</span></code>, used for tuning
<code class="docutils literal"><span class="pre">adaptive_memory_usage_constant</span></code>.</p>
</div></blockquote>
<p><strong>sorting_keys</strong> : List[Tuple[str, str]]</p>
<blockquote>
<div><p>See <code class="xref py py-class docutils literal"><span class="pre">BucketIterator</span></code>.</p>
</div></blockquote>
<p><strong>padding_noise</strong> : List[Tuple[str, str]]</p>
<blockquote class="last">
<div><p>See <code class="xref py py-class docutils literal"><span class="pre">BucketIterator</span></code>.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="allennlp.data.iterators.adaptive_iterator.AdaptiveIterator.from_params">
<em class="property">classmethod </em><code class="descname">from_params</code><span class="sig-paren">(</span><em>params: allennlp.common.params.Params</em><span class="sig-paren">)</span> &#x2192; allennlp.data.iterators.adaptive_iterator.AdaptiveIterator<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/adaptive_iterator.py#L140-L157"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.adaptive_iterator.AdaptiveIterator.from_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.iterators.adaptive_iterator.AdaptiveIterator.get_num_batches">
<code class="descname">get_num_batches</code><span class="sig-paren">(</span><em>dataset: allennlp.data.dataset.Dataset</em><span class="sig-paren">)</span> &#x2192; int<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/adaptive_iterator.py#L95-L102"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.adaptive_iterator.AdaptiveIterator.get_num_batches" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a non-trivial operation with an <code class="docutils literal"><span class="pre">AdaptiveIterator</span></code>, and it&#8217;s only approximate,
because the actual number of batches constructed depends on the padding noise.  Call this
sparingly.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.iterators.basic_iterator"><span id="basic-iterator"></span></span><dl class="class">
<dt id="allennlp.data.iterators.basic_iterator.BasicIterator">
<em class="property">class </em><code class="descclassname">allennlp.data.iterators.basic_iterator.</code><code class="descname">BasicIterator</code><span class="sig-paren">(</span><em>batch_size: int = 32</em><span class="sig-paren">)</span> &#x2192; None<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/basic_iterator.py#L15-L47"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.basic_iterator.BasicIterator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.iterators.data_iterator.DataIterator" title="allennlp.data.iterators.data_iterator.DataIterator"><code class="xref py py-class docutils literal"><span class="pre">allennlp.data.iterators.data_iterator.DataIterator</span></code></a></p>
<p>A very basic iterator, which takes a dataset, pads all of its instances to the maximum lengths
of the relevant fields across the whole dataset, and yields fixed size batches.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>batch_size</strong> : int, optional, (default = 32)</p>
<blockquote class="last">
<div><p>The size of each batch of instances yielded when calling the iterator.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="allennlp.data.iterators.basic_iterator.BasicIterator.from_params">
<em class="property">classmethod </em><code class="descname">from_params</code><span class="sig-paren">(</span><em>params: allennlp.common.params.Params</em><span class="sig-paren">)</span> &#x2192; allennlp.data.iterators.basic_iterator.BasicIterator<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/basic_iterator.py#L43-L47"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.basic_iterator.BasicIterator.from_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.data.iterators.basic_iterator.BasicIterator.get_num_batches">
<code class="descname">get_num_batches</code><span class="sig-paren">(</span><em>dataset: allennlp.data.dataset.Dataset</em><span class="sig-paren">)</span> &#x2192; int<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/basic_iterator.py#L28-L30"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.basic_iterator.BasicIterator.get_num_batches" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of batches that <code class="docutils literal"><span class="pre">dataset</span></code> will be split into; if you want to track
progress through the batch with the generator produced by <code class="docutils literal"><span class="pre">__call__</span></code>, this could be
useful.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-allennlp.data.iterators.bucket_iterator"><span id="bucket-iterator"></span></span><dl class="class">
<dt id="allennlp.data.iterators.bucket_iterator.BucketIterator">
<em class="property">class </em><code class="descclassname">allennlp.data.iterators.bucket_iterator.</code><code class="descname">BucketIterator</code><span class="sig-paren">(</span><em>sorting_keys: typing.List[typing.Tuple[str</em>, <em>str]] = None</em>, <em>padding_noise: float = 0.1</em>, <em>biggest_batch_first: bool = False</em>, <em>batch_size: int = 32</em><span class="sig-paren">)</span> &#x2192; None<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/bucket_iterator.py#L14-L113"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.bucket_iterator.BucketIterator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#allennlp.data.iterators.basic_iterator.BasicIterator" title="allennlp.data.iterators.basic_iterator.BasicIterator"><code class="xref py py-class docutils literal"><span class="pre">allennlp.data.iterators.basic_iterator.BasicIterator</span></code></a></p>
<p>An iterator which by default, pads batches with respect to the maximum input lengths <cite>per
batch</cite>. Additionally, you can provide a list of field names and padding keys which the dataset
will be sorted by before doing this batching, causing inputs with similar length to be batched
together, making computation more efficient (as less time is wasted on padded elements of the
batch).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>sorting_keys</strong> : List[Tuple[str, str]], optional (default = [])</p>
<blockquote>
<div><p>To bucket inputs into batches, we want to group the instances by padding length, so that we
minimize the amount of padding necessary per batch. In order to do this, we need to know
which fields need what type of padding, and in what order.</p>
<p>For example, <code class="docutils literal"><span class="pre">[(&quot;sentence1&quot;,</span> <span class="pre">&quot;num_tokens&quot;),</span> <span class="pre">(&quot;sentence2&quot;,</span> <span class="pre">&quot;num_tokens&quot;),</span> <span class="pre">(&quot;sentence1&quot;,</span>
<span class="pre">&quot;num_token_characters&quot;)]</span></code> would sort a dataset first by the &#8220;num_tokens&#8221; of the
&#8220;sentence1&#8221; field, then by the &#8220;num_tokens&#8221; of the &#8220;sentence2&#8221; field, and finally by the
&#8220;num_token_characters&#8221; of the &#8220;sentence1&#8221; field.  TODO(mattg): we should have some
documentation somewhere that gives the standard padding keys used by different fields.</p>
<p>By default, the list of sorting keys is empty, meaning the dataset won&#8217;t be sorted and
batches will just be padded using the max lengths of all fields requiring padding
calculated per batch.</p>
</div></blockquote>
<p><strong>padding_noise</strong> : float, optional (default=.1)</p>
<blockquote>
<div><p>When sorting by padding length, we add a bit of noise to the lengths, so that the sorting
isn&#8217;t deterministic.  This parameter determines how much noise we add, as a percentage of
the actual padding value for each instance.</p>
</div></blockquote>
<p><strong>biggest_batch_first</strong> : bool, optional (default=False)</p>
<blockquote>
<div><p>This is largely for testing, to see how large of a batch you can safely use with your GPU.
This will let you try out the largest batch that you have in the data <cite>first</cite>, so that if
you&#8217;re going to run out of memory, you know it early, instead of waiting through the whole
epoch to find out at the end that you&#8217;re going to crash.</p>
</div></blockquote>
<p><strong>batch_size</strong> : int, optional, (default = 32)</p>
<blockquote class="last">
<div><p>The size of each batch of instances yielded when calling the iterator.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="allennlp.data.iterators.bucket_iterator.BucketIterator.from_params">
<em class="property">classmethod </em><code class="descname">from_params</code><span class="sig-paren">(</span><em>params: allennlp.common.params.Params</em><span class="sig-paren">)</span> &#x2192; allennlp.data.iterators.bucket_iterator.BucketIterator<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/bucket_iterator.py#L103-L113"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.data.iterators.bucket_iterator.BucketIterator.from_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="allennlp.data.token_indexers.html" class="btn btn-neutral float-right" title="allennlp.data.token_indexers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="allennlp.data.instance.html" class="btn btn-neutral" title="allennlp.data.instance" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Allen Institute for Artificial Intelligence.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>