

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>allennlp.models.coreference_resolution &mdash; AllenNLP 0.2.2 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="AllenNLP 0.2.2 documentation" href="../index.html"/>
        <link rel="up" title="allennlp.models" href="allennlp.models.html"/>
        <link rel="next" title="allennlp.models.semantic_role_labeler" href="allennlp.models.semantic_role_labeler.html"/>
        <link rel="prev" title="allennlp.models.reading_comprehension" href="allennlp.models.reading_comprehension.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/allennlp-logo-dark.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                0.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="allennlp.commands.html">allennlp.commands</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.subcommand.html">allennlp.commands.subcommand</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.evaluate.html">allennlp.commands.evaluate</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.predict.html">allennlp.commands.predict</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.serve.html">allennlp.commands.serve</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.commands.train.html">allennlp.commands.train</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.common.html">allennlp.common</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.checks.html">allennlp.common.checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.file_utils.html">allennlp.common.file_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.params.html">allennlp.common.params</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.registrable.html">allennlp.common.registrable</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.squad_eval.html">allennlp.common.squad_eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.tee_logger.html">allennlp.common.tee_logger</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.testing.html">allennlp.common.testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.common.util.html">allennlp.common.util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.data.html">allennlp.data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.dataset.html">allennlp.data.dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.dataset_readers.html">allennlp.data.dataset_readers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.dataset_reader.html">allennlp.data.dataset_readers.dataset_reader</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.conll2003.html">allennlp.data.dataset_readers.conll2003</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.language_modeling.html">allennlp.data.dataset_readers.language_modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.reading_comprehension.html">allennlp.data.dataset_readers.reading_comprehension</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.coreference_resolution.html">allennlp.data.dataset_readers.coreference_resolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.semantic_role_labeling.html">allennlp.data.dataset_readers.semantic_role_labeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.seq2seq.html">allennlp.data.dataset_readers.seq2seq</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.sequence_tagging.html">allennlp.data.dataset_readers.sequence_tagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="allennlp.data.dataset_readers.snli.html">allennlp.data.dataset_readers.snli</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.fields.html">allennlp.data.fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.instance.html">allennlp.data.instance</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.iterators.html">allennlp.data.iterators</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.token_indexers.html">allennlp.data.token_indexers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.tokenizers.html">allennlp.data.tokenizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.data.vocabulary.html">allennlp.data.vocabulary</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="allennlp.models.html">allennlp.models</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.model.html">allennlp.models.model</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.archival.html">allennlp.models.archival</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.crf_tagger.html">allennlp.models.crf_tagger</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.decomposable_attention.html">allennlp.models.decomposable_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.encoder_decoders.html">allennlp.models.encoder_decoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.reading_comprehension.html">allennlp.models.reading_comprehension</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">allennlp.models.coreference_resolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.semantic_role_labeler.html">allennlp.models.semantic_role_labeler</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.models.simple_tagger.html">allennlp.models.simple_tagger</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.modules.html">allennlp.modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.attention.html">allennlp.modules.attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.augmented_lstm.html">allennlp.modules.augmented_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.lstm_cell_with_projection.html">allennlp.modules.lstm_cell_with_projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.elmo.html">allennlp.modules.elmo</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.elmo_lstm.html">allennlp.modules.elmo_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.conditional_random_field.html">allennlp.modules.conditional_random_field</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.feedforward.html">allennlp.modules.feedforward</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.highway.html">allennlp.modules.highway</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.matrix_attention.html">allennlp.modules.matrix_attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2seq_encoders.html">allennlp.modules.seq2seq_encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.seq2vec_encoders.html">allennlp.modules.seq2vec_encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.similarity_functions.html">allennlp.modules.similarity_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.stacked_alternating_lstm.html">allennlp.modules.stacked_alternating_lstm</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.text_field_embedders.html">allennlp.modules.text_field_embedders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.time_distributed.html">allennlp.modules.time_distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.token_embedders.html">allennlp.modules.token_embedders</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.modules.scalar_mix.html">allennlp.modules.scalar_mix</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.nn.html">allennlp.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.activations.html">allennlp.nn.activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.initializers.html">allennlp.nn.initializers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.regularizers.html">allennlp.nn.regularizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.nn.util.html">allennlp.nn.util</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.service.html">allennlp.service</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.db.html">allennlp.service.db</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.permalinks.html">allennlp.service.permalinks</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.predictors.html">allennlp.service.predictors</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.service.server_sanic.html">allennlp.service.server_sanic</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="allennlp.training.html">allennlp.training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.learning_rate_schedulers.html">allennlp.training.learning_rate_schedulers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.metrics.html">allennlp.training.metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.optimizers.html">allennlp.training.optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="allennlp.training.trainer.html">allennlp.training.trainer</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">AllenNLP</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="allennlp.models.html">allennlp.models</a> &raquo;</li>
        
      <li>allennlp.models.coreference_resolution</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api/allennlp.models.coreference_resolution.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-allennlp.models.coreference_resolution">
<span id="allennlp-models-coreference-resolution"></span><h1>allennlp.models.coreference_resolution<a class="headerlink" href="#module-allennlp.models.coreference_resolution" title="Permalink to this headline">¶</a></h1>
<span class="target" id="module-allennlp.models.coreference_resolution.coref"></span><dl class="class">
<dt id="allennlp.models.coreference_resolution.coref.CoreferenceResolver">
<em class="property">class </em><code class="descclassname">allennlp.models.coreference_resolution.coref.</code><code class="descname">CoreferenceResolver</code><span class="sig-paren">(</span><em>vocab: allennlp.data.vocabulary.Vocabulary</em>, <em>text_field_embedder: allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder</em>, <em>context_layer: allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder</em>, <em>mention_feedforward: allennlp.modules.feedforward.FeedForward</em>, <em>antecedent_feedforward: allennlp.modules.feedforward.FeedForward</em>, <em>feature_size: int</em>, <em>max_span_width: int</em>, <em>spans_per_word: float</em>, <em>max_antecedents: int</em>, <em>lexical_dropout: float = 0.2</em>, <em>initializer: allennlp.nn.initializers.InitializerApplicator = &lt;allennlp.nn.initializers.InitializerApplicator object&gt;</em>, <em>regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator</em>, <em>NoneType] = None</em><span class="sig-paren">)</span> &#x2192; None<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/models/coreference_resolution/coref.py#L23-L796"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.models.coreference_resolution.coref.CoreferenceResolver" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="allennlp.models.model.html#allennlp.models.model.Model" title="allennlp.models.model.Model"><code class="xref py py-class docutils literal"><span class="pre">allennlp.models.model.Model</span></code></a></p>
<p>This <code class="docutils literal"><span class="pre">Model</span></code> implements the coreference resolution model described &#8220;End-to-end Neural
Coreference Resolution&#8221;
&lt;<a class="reference external" href="https://www.semanticscholar.org/paper/End-to-end-Neural-Coreference-Resolution-Lee-He/3f2114893dc44eacac951f148fbff142ca200e83">https://www.semanticscholar.org/paper/End-to-end-Neural-Coreference-Resolution-Lee-He/3f2114893dc44eacac951f148fbff142ca200e83</a>&gt;
by Lee et al., 2017.
The basic outline of this model is to get an embedded representation of each span in the
document. These span representations are scored and used to prune away spans that are unlikely
to occur in a coreference cluster. For the remaining spans, the model decides which antecedent
span (if any) they are coreferent with. The resulting coreference links, after applying
transitivity, imply a clustering of the spans in the document.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>vocab</strong> : <code class="docutils literal"><span class="pre">Vocabulary</span></code></p>
<p><strong>text_field_embedder</strong> : <code class="docutils literal"><span class="pre">TextFieldEmbedder</span></code></p>
<blockquote>
<div><p>Used to embed the <code class="docutils literal"><span class="pre">text</span></code> <code class="docutils literal"><span class="pre">TextField</span></code> we get as input to the model.</p>
</div></blockquote>
<p><strong>context_layer</strong> : <code class="docutils literal"><span class="pre">Seq2SeqEncoder</span></code></p>
<blockquote>
<div><p>This layer incorporates contextual information for each word in the document.</p>
</div></blockquote>
<p><strong>mention_feedforward</strong> : <code class="docutils literal"><span class="pre">FeedForward</span></code></p>
<blockquote>
<div><p>This feedforward network is applied to the span representations which is then scored
by a linear layer.</p>
</div></blockquote>
<p><strong>antecedent_feedforward: ``FeedForward``</strong></p>
<blockquote>
<div><p>This feedforward network is applied to pairs of span representation, along with any
pairwise features, which is then scored by a linear layer.</p>
</div></blockquote>
<p><strong>feature_size: ``int``</strong></p>
<blockquote>
<div><p>The embedding size for all the embedded features, such as distances or span widths.</p>
</div></blockquote>
<p><strong>max_span_width: ``int``</strong></p>
<blockquote>
<div><p>The maximum width of candidate spans.</p>
</div></blockquote>
<p><strong>spans_per_word: float, required.</strong></p>
<blockquote>
<div><p>A multiplier between zero and one which controls what percentage of candidate mention
spans we retain with respect to the number of words in the document.</p>
</div></blockquote>
<p><strong>max_antecedents: int, required.</strong></p>
<blockquote>
<div><p>For each mention which survives the pruning stage, we consider this many antecedents.</p>
</div></blockquote>
<p><strong>lexical_dropout: ``int``</strong></p>
<blockquote>
<div><p>The probability of dropping out dimensions of the embedded text.</p>
</div></blockquote>
<p><strong>initializer</strong> : <code class="docutils literal"><span class="pre">InitializerApplicator</span></code>, optional (default=``InitializerApplicator()``)</p>
<blockquote>
<div><p>Used to initialize the model parameters.</p>
</div></blockquote>
<p><strong>regularizer</strong> : <code class="docutils literal"><span class="pre">RegularizerApplicator</span></code>, optional (default=``None``)</p>
<blockquote class="last">
<div><p>If provided, will be used to calculate the regularization penalty during training.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="allennlp.models.coreference_resolution.coref.CoreferenceResolver.decode">
<code class="descname">decode</code><span class="sig-paren">(</span><em>output_dict: typing.Dict[str, torch.FloatTensor]</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/models/coreference_resolution/coref.py#L314-L390"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.models.coreference_resolution.coref.CoreferenceResolver.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the list of spans and predicted antecedent indices into clusters
of spans for each element in the batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>output_dict</strong> : <code class="docutils literal"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>, required.</p>
<blockquote>
<div><p>The result of calling <a class="reference internal" href="#allennlp.models.coreference_resolution.coref.CoreferenceResolver.forward" title="allennlp.models.coreference_resolution.coref.CoreferenceResolver.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a> on an instance or batch of instances.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The same output dictionary, but with an additional <code class="docutils literal"><span class="pre">clusters</span></code> key:</p>
<p><strong>clusters</strong> : <code class="docutils literal"><span class="pre">List[List[List[Tuple[int,</span> <span class="pre">int]]]]</span></code></p>
<blockquote class="last">
<div><p>A nested list, representing, for each instance in the batch, the list of clusters,
which are in turn comprised of a list of (start, end) inclusive spans into the
original document.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="allennlp.models.coreference_resolution.coref.CoreferenceResolver.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>text: typing.Dict[str, torch.LongTensor], span_starts: torch.IntTensor, span_ends: torch.IntTensor, span_labels: torch.IntTensor = None, metadata: typing.List[typing.Dict[str, typing.Any]] = None</em><span class="sig-paren">)</span> &#x2192; typing.Dict[str, torch.FloatTensor]<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/models/coreference_resolution/coref.py#L104-L312"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.models.coreference_resolution.coref.CoreferenceResolver.forward" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>text</strong> : <code class="docutils literal"><span class="pre">Dict[str,</span> <span class="pre">torch.LongTensor]</span></code>, required.</p>
<blockquote>
<div><p>The output of a <code class="docutils literal"><span class="pre">TextField</span></code> representing the text of
the document.</p>
</div></blockquote>
<p><strong>span_starts</strong> : <code class="docutils literal"><span class="pre">torch.IntTensor</span></code>, required.</p>
<blockquote>
<div><p>A tensor of shape (batch_size, num_spans, 1), representing the start indices of
candidate spans for mentions. Comes from a <code class="docutils literal"><span class="pre">ListField[IndexField]</span></code> of indices into
the text of the document.</p>
</div></blockquote>
<p><strong>span_ends</strong> : <code class="docutils literal"><span class="pre">torch.IntTensor</span></code>, required.</p>
<blockquote>
<div><p>A tensor of shape (batch_size, num_spans, 1), representing the end indices of
candidate spans for mentions. Comes from a <code class="docutils literal"><span class="pre">ListField[IndexField]</span></code> of indices into
the text of the document.</p>
</div></blockquote>
<p><strong>span_labels</strong> : <code class="docutils literal"><span class="pre">torch.IntTensor</span></code>, optional (default = None)</p>
<blockquote>
<div><p>A tensor of shape (batch_size, num_spans), representing the cluster ids
of each span, or -1 for those which do not appear in any clusters.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">An output dictionary consisting of:</p>
<p><strong>top_spans</strong> : <code class="docutils literal"><span class="pre">torch.IntTensor</span></code></p>
<blockquote>
<div><p>A tensor of shape <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">num_spans_to_keep,</span> <span class="pre">2)</span></code> representing
the start and end word indices of the top spans that survived the pruning stage.</p>
</div></blockquote>
<p><strong>antecedent_indices</strong> : <code class="docutils literal"><span class="pre">torch.IntTensor</span></code></p>
<blockquote>
<div><p>A tensor of shape <code class="docutils literal"><span class="pre">(num_spans_to_keep,</span> <span class="pre">max_antecedents)</span></code> representing for each top span
the index (with respect to top_spans) of the possible antecedents the model considered.</p>
</div></blockquote>
<p><strong>predicted_antecedents</strong> : <code class="docutils literal"><span class="pre">torch.IntTensor</span></code></p>
<blockquote>
<div><p>A tensor of shape <code class="docutils literal"><span class="pre">(batch_size,</span> <span class="pre">num_spans_to_keep)</span></code> representing, for each top span, the
index (with respect to antecedent_indices) of the most likely antecedent. -1 means there
was no predicted link.</p>
</div></blockquote>
<p><strong>loss</strong> : <code class="docutils literal"><span class="pre">torch.FloatTensor</span></code>, optional</p>
<blockquote class="last">
<div><p>A scalar loss to be optimised.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="allennlp.models.coreference_resolution.coref.CoreferenceResolver.from_params">
<em class="property">classmethod </em><code class="descname">from_params</code><span class="sig-paren">(</span><em>vocab: allennlp.data.vocabulary.Vocabulary</em>, <em>params: allennlp.common.params.Params</em><span class="sig-paren">)</span> &#x2192; allennlp.models.coreference_resolution.coref.CoreferenceResolver<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/models/coreference_resolution/coref.py#L763-L796"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.models.coreference_resolution.coref.CoreferenceResolver.from_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="allennlp.models.coreference_resolution.coref.CoreferenceResolver.get_metrics">
<code class="descname">get_metrics</code><span class="sig-paren">(</span><em>reset: bool = False</em><span class="sig-paren">)</span> &#x2192; typing.Dict[str, float]<a class="reference external" href="http://github.com/allenai/allennlp/blob/master/allennlp/models/coreference_resolution/coref.py#L392-L400"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#allennlp.models.coreference_resolution.coref.CoreferenceResolver.get_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary of metrics. This method will be called by
<code class="xref py py-class docutils literal"><span class="pre">allennlp.training.Trainer</span></code> in order to compute and use model metrics for early
stopping and model serialisation.  We return an empty dictionary here rather than raising
as it is not required to implement metrics for a new model.  A boolean <cite>reset</cite> parameter is
passed, as frequently a metric accumulator will have some state which should be reset
between epochs. This is also compatible with <code class="xref py py-class docutils literal"> <span class="pre">Metrics</span>
<span class="pre">should</span> <span class="pre">be</span> <span class="pre">populated</span> <span class="pre">during</span> <span class="pre">the</span> <span class="pre">call</span> <span class="pre">to</span> <span class="pre">``forward`</span></code>, with the
<code class="xref py py-class docutils literal"><span class="pre">Metric</span></code> handling the accumulation of the metric until this
method is called.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="allennlp.models.semantic_role_labeler.html" class="btn btn-neutral float-right" title="allennlp.models.semantic_role_labeler" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="allennlp.models.reading_comprehension.html" class="btn btn-neutral" title="allennlp.models.reading_comprehension" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Allen Institute for Artificial Intelligence.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.2.2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>